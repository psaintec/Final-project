{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - file analyse Word 2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks to this code we will be able to define the 20 most common words use in a file text.\n",
    "    1) The first step will be to clean the text (Natural Language Processing, lemmatize, elimate stopwords & punctuation, etc)\n",
    "    2) Then, we create a graph to be use in Gephi\n",
    "    \n",
    "### We will realize this work with a nlp_sm (small base of words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import en_core_web_sm\n",
    "import en_core_web_lg\n",
    "import math\n",
    "import token\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import gensim #W2V\n",
    "\n",
    "# Pour générer un graphe Gephy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sm = en_core_web_sm.load()\n",
    "nlp_sm.max_length = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "numbers = string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Files/Anaerobic digestion for the stabilization of the organic fraction of municipal waste.txt') as fp:\n",
    "    text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Files/Anaerobic digestion of food waste - thesis.txt') as fp:\n",
    "    text += fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Files/What a Waste 2.0 A Global Snapshot of Solid Waste Management.txt') as fp:\n",
    "    text += fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function split a text into a list of sentences.\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\\n\\n\\n\\n\\n\",\"<stop>\")\n",
    "    text = text.replace(\"\\n\\n\\n\\n\\n\",\"<stop>\")\n",
    "    text = text.replace(\"\\n\\n\\n\\n\",\"<stop>\")\n",
    "    text = text.replace(\"\\n\\n\\n\",\"<stop>\")\n",
    "    text = text.replace(\"\\n\\n\",\"<stop>\")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\". \",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function split and clean each sentences of a list into a list of words.\n",
    "def cleanup_sentences_sm(doc):\n",
    "    words_list_sm = []\n",
    "    for i in doc:\n",
    "        sentence = nlp_sm(i)\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in sentence if tok.lemma_ != '-PRON-'] #lemmatize\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords] #delete stopwords\n",
    "        tokens = [tok for tok in tokens if tok not in punctuations] #delete punctuation\n",
    "        tokens = [tok for tok in tokens if not tok.isdigit()] #delete integers\n",
    "        tokens = [tok for tok in tokens if len(tok) > 2] #delete letters alone\n",
    "        tokens = [tok for tok in tokens if tok != 'et'] #delete the word 'et'\n",
    "        tokens = [tok for tok in tokens if tok != 'al'] #delete the word 'al'\n",
    "        tokens = [tok for tok in tokens if tok != 'angelidaki'] #delete the word 'angelidaki'\n",
    "        tokens = [tok for tok in tokens if tok != 'kim'] #delete the word 'kim'\n",
    "        tokens = [tok for tok in tokens if tok != 'epa'] #delete the word 'epa'\n",
    "        tokens = [tok for tok in tokens if tok != 'ostrem'] #delete the word 'ostrem'\n",
    "        tokens = [tok for tok in tokens if tok != 'dranco'] #delete the word 'dranco'\n",
    "        tokens = [tok for tok in tokens if tok != 'dufferin'] #delete the word 'dufferin'\n",
    "        tokens = [tok for tok in tokens if tok != 'garcia'] #delete the word 'garcia'\n",
    "        tokens = [tok for tok in tokens if tok != 'del'] #delete the word 'del'\n",
    "        tokens = [tok for tok in tokens if tok != 'kayhanian'] #delete the word 'dranco'\n",
    "        tokens = [tok for tok in tokens if tok != 'vss'] #delete the word 'dufferin'\n",
    "        tokens = ' '.join(tokens)\n",
    "        word_list = [word.strip(string.punctuation) for word in tokens.split()] #create a list with each word\n",
    "\n",
    "        #delete decimals and **/**\n",
    "        to_delete = []\n",
    "        for each in word_list:\n",
    "            if re.search(\"^[0-9]*.[0-9]*$\", each) is not None:\n",
    "                to_delete.append(each)\n",
    "            elif re.search(\"^[a-z0-9]*/[a-z0-9]*$\", each) is not None:\n",
    "                to_delete.append(each)\n",
    "        for each in to_delete:\n",
    "            word_list.remove(each)\n",
    "        \n",
    "        words_list_sm.append(word_list)\n",
    "\n",
    "    return words_list_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function clean a text into a list of words.\n",
    "def cleanup_text_sm(doc):\n",
    "    doc = nlp_sm(doc)    \n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-'] #lemmatize\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords] #delete stopwords\n",
    "    tokens = [tok for tok in tokens if tok not in punctuations] #delete punctuation\n",
    "    tokens = [tok for tok in tokens if not tok.isdigit()] #delete integers\n",
    "    tokens = [tok for tok in tokens if len(tok) > 2] #delete letters alone\n",
    "    tokens = [tok for tok in tokens if tok != 'et'] #delete the word 'et'\n",
    "    tokens = [tok for tok in tokens if tok != 'al'] #delete the word 'al'\n",
    "    tokens = [tok for tok in tokens if tok != 'angelidaki'] #delete the word 'angelidaki'\n",
    "    tokens = [tok for tok in tokens if tok != 'kim'] #delete the word 'kim'\n",
    "    tokens = [tok for tok in tokens if tok != 'epa'] #delete the word 'epa'\n",
    "    tokens = [tok for tok in tokens if tok != 'ostrem'] #delete the word 'ostrem'\n",
    "    tokens = [tok for tok in tokens if tok != 'dranco'] #delete the word 'dranco'\n",
    "    tokens = [tok for tok in tokens if tok != 'dufferin'] #delete the word 'dufferin'\n",
    "    tokens = [tok for tok in tokens if tok != 'garcia'] #delete the word 'garcia'\n",
    "    tokens = [tok for tok in tokens if tok != 'del'] #delete the word 'del'\n",
    "    tokens = [tok for tok in tokens if tok != 'kayhanian'] #delete the word 'dranco'\n",
    "    tokens = [tok for tok in tokens if tok != 'vss'] #delete the word 'dufferin'\n",
    "    tokens = ' '.join(tokens)\n",
    "    words_list = [word.strip(string.punctuation) for word in tokens.split()] #create a list with each word\n",
    "\n",
    "    #delete decimals\n",
    "    to_delete = []\n",
    "    for each in words_list:\n",
    "        if re.search(\"^[0-9]*.[0-9]*$\", each) is not None:\n",
    "            to_delete.append(each)\n",
    "        elif re.search(\"^[a-z0-9]*/[a-z0-9]*$\", each) is not None:\n",
    "            to_delete.append(each)\n",
    "    for each in to_delete:\n",
    "        words_list.remove(each)\n",
    "            \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function delete all the words which occure less than a number we define from a list of list.  \n",
    "def most_common_words(list_W2V, doc, occurence):\n",
    "    to_delete = []\n",
    "    sentence_list = []\n",
    "    counts = dict(Counter(doc))\n",
    "    for word in counts.keys():\n",
    "        if counts[word] <= occurence:\n",
    "            to_delete.append(word)\n",
    "    for sentence in list_W2V:\n",
    "        word_list = []\n",
    "        for word in sentence:\n",
    "            if not word in to_delete:\n",
    "                word_list.append(word)\n",
    "        sentence_list.append(word_list)\n",
    "    list_W2V = [each for each in sentence_list if len(each) > 0]\n",
    "    \n",
    "    return list_W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(list_W2V):\n",
    "    model_sm = gensim.models.Word2Vec(list_W2V, min_count = 5, size = 300)\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(list(model_sm.wv.vocab))\n",
    "    for word in model_sm.wv.vocab:\n",
    "        for s in model_sm.most_similar(word):\n",
    "            graph.add_edge(word, s[0], weigh = s[1])\n",
    "    print(nx.info(graph))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_sm = split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_sm = cleanup_sentences_sm(sentences_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sm = cleanup_text_sm(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_list_sm = most_common_words(W2V_sm, words_sm, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of a graph file with only the words which occure more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 71\n",
      "Number of edges: 537\n",
      "Average degree:  15.1268\n"
     ]
    }
   ],
   "source": [
    "graph_occurences100 = create_graph(W2V_list_sm)\n",
    "nx.write_gexf(graph_occurences100,'graphs/graph_occurences100_bis.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse graph_occurences100\n",
    "\n",
    "The graph is available in the directory graphs in pdf or PNG. You can open the PNG by launching the next cell.\n",
    "\n",
    "To do this graph, I used W2V, which define the similarity between words. I analyzed only the words which occure 100 or more times in the text.\n",
    "Then on gephy, I used the spatialization \"Force Atlas 2\" to define the shape of the graph. Thanks to the statistical tool modularity, Gephy computed clusters and he found 4 groups of words. They are well separated in the graph.\n",
    "The words which occure the most (according to the other study about Word_occurence) have a similarity less important than other, they appear in orange in the graph and they are really smaller than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "myImage = Image.open(\"graphs/Graph_occurences100.PNG\");\n",
    "myImage.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of a graph file with all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1637\n",
      "Number of edges: 16013\n",
      "Average degree:  19.5638\n"
     ]
    }
   ],
   "source": [
    "graph_all = create_graph(W2V_sm)\n",
    "nx.write_gexf(graph_all,'graphs/graph_all_bis.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse graph_all\n",
    "\n",
    "The graph is available in the directory graphs in pdf or PNG. You can open the PNG by launching the next cell.\n",
    "\n",
    "To do this graph, I used the same method that previously. This time, I analyzed all the words of the text, except the one that I cleaned. About the cleaning part, my code is really far away from being perfect. There is still some acronyms, references names and other stuff I would like to eliminate in order to make the text more clear.\n",
    "\n",
    "Then on gephy, I used exactly the same method with \"Force Atlas 2\" spatialization and modularity to get clusters. Gephy found 5 groups of words.\n",
    "The shape of the graph is quite elongated. There is three major clusters the blue, the light green and the dark green. At the top, there is the blue cluster, in the middle is the light green group and the dark green one is really at the bottom of the graph and represent the words which occure the most in the text, like waste, anaerobic, digestion etc. This position on the graph confirm that the similarity of these words are really low. They are used together and don't refer to many others.\n",
    "The purple and orange clusters are less concentrated and seems to be linked with many other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "myImage = Image.open(\"graphs/Graph_all.PNG\");\n",
    "myImage.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
